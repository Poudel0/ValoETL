{"cells":[{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":["import requests\n","import json\n","import os\n","import pandas as pd\n","import time\n","import re"]},{"cell_type":"markdown","metadata":{},"source":["Define the URL for the tournament<br>\n","series_url='https://www.rib.gg/series/52003'"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[],"source":["tourney_url = \"https://www.rib.gg/events/champions-tour-2023-americas-last-chance-qualifier/matches/2917\""]},{"cell_type":"markdown","metadata":{},"source":["File to store scraped seriesIds"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[],"source":["scraped_series_ids_file = 'scraped_series_ids.txt'"]},{"cell_type":"markdown","metadata":{},"source":["Function to sanitize filenames and directory names"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[],"source":["def sanitize_filename(filename):\n","    return re.sub(r'[<>:\"/\\\\|?*]', '', filename)"]},{"cell_type":"markdown","metadata":{},"source":["Function to load scraped seriesIds from file"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[],"source":["def load_scraped_series_ids():\n","    if os.path.exists(scraped_series_ids_file):\n","        with open(scraped_series_ids_file, 'r') as file:\n","            return file.read().splitlines()\n","    else:\n","        return []"]},{"cell_type":"markdown","metadata":{},"source":["Function to save scraped seriesIds to file"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[],"source":["def save_scraped_series_ids(scraped_series_ids):\n","    with open(scraped_series_ids_file, 'w') as file:\n","        for series_id in scraped_series_ids:\n","            file.write(f\"{series_id}\\n\")"]},{"cell_type":"markdown","metadata":{},"source":["Load scraped seriesIds from file"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[],"source":["scraped_series_ids = load_scraped_series_ids()"]},{"cell_type":"markdown","metadata":{},"source":["Function to extract JSON data from HTML"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[],"source":["def extract_json_data(response_text):\n","    start_index = response_text.find('<script id=\"__NEXT_DATA__\" type=\"application/json\">')\n","    start_index += len('<script id=\"__NEXT_DATA__\" type=\"application/json\">')\n","    end_index = response_text.find('</script>', start_index)\n","    json_string = response_text[start_index:end_index].strip()\n","    return json.loads(json_string)"]},{"cell_type":"markdown","metadata":{},"source":["Function to get series header data and match IDs"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[],"source":["def SeriesHeader(seriesId):\n","    print(f\"Fetching series header data for series ID: {seriesId}\")\n","    url = f'https://www.rib.gg/series/{seriesId}'\n","    try:\n","        response = requests.get(url)\n","        response.raise_for_status()\n","    except requests.RequestException as e:\n","        print(f'Failed trying to scrape series header: {e}')\n","        return None, None\n","    stats_data_raw = extract_json_data(response.text)\n","    matches = stats_data_raw['props']['pageProps']['series']['matches']\n","    match_ids = [match['id'] for match in matches]\n","    return stats_data_raw, match_ids"]},{"cell_type":"markdown","metadata":{},"source":["Function to update IGN and ID data"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[],"source":["def update_ign_and_id(stats_data):\n","    print(\"Updating IGN and ID data\")\n","    csv_file = 'ignfile.csv'\n","    \n","    matches = stats_data['props']['pageProps']['series']['matches']\n","    ids, igns = [], []\n","    for match in matches:\n","        players = match['players']\n","        for player_obj in players:\n","            ids.append(player_obj['player']['id'])\n","            igns.append(player_obj['player']['ign'])\n","    df = pd.DataFrame({'id': ids, 'name': igns}).drop_duplicates(subset=['id', 'name'])\n","    if os.path.exists(csv_file) and os.path.getsize(csv_file) > 0:\n","        existing_df = pd.read_csv(csv_file)\n","    else:\n","        existing_df = pd.DataFrame()\n","    combined_df = pd.concat([existing_df, df], ignore_index=True).drop_duplicates(subset=['id', 'name'])\n","    try:\n","        combined_df.to_csv(csv_file, index=False)\n","        print(f\"IGN and ID data saved to {csv_file}\")\n","    except Exception as e:\n","        print(f\"Failed to save IGN and ID data: {e}\")"]},{"cell_type":"markdown","metadata":{},"source":["Function to update team data"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[],"source":["def update_team(stats_data):\n","    print(\"Updating team data\")\n","    csv_file = 'teamfile.csv'\n","    keys = ['id', 'name', 'shortName']\n","    t1 = stats_data['props']['pageProps']['series']['team1']\n","    t2 = stats_data['props']['pageProps']['series']['team2']\n","    df1 = pd.DataFrame([t1], columns=keys).drop_duplicates(subset=keys)\n","    df2 = pd.DataFrame([t2], columns=keys).drop_duplicates(subset=keys)\n","    if os.path.exists(csv_file) and os.path.getsize(csv_file) > 0:\n","        existing_df = pd.read_csv(csv_file)\n","    else:\n","        existing_df = pd.DataFrame()\n","    combined_df = pd.concat([existing_df, df1, df2], ignore_index=True).drop_duplicates(subset=keys)\n","    try:\n","        combined_df.to_csv(csv_file, index=False)\n","        print(f\"Team data saved to {csv_file}\")\n","    except Exception as e:\n","        print(f\"Failed to save team data: {e}\")"]},{"cell_type":"markdown","metadata":{},"source":["Function to update abilities data"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[],"source":["def update_abilities(stats_data):\n","    print(\"Updating abilities data\")\n","    csv_file = 'abilities.csv'\n","    abilities = stats_data['props']['pageProps']['content']['abilities']\n","    ids, names, types, agentId, damages = [], [], [], [], []\n","    for agent in abilities:\n","        ids.append(agent['id'])\n","        names.append(agent['name'])\n","        types.append(agent['type'])\n","        agentId.append(agent['agentId'])\n","        damages.append(agent['damages'])\n","    df = pd.DataFrame({'id': ids, 'name': names, 'type': types, 'agentId': agentId, 'damages': damages})\n","    if os.path.exists(csv_file) and os.path.getsize(csv_file) > 0:\n","        existing_df = pd.read_csv(csv_file)\n","    else:\n","        existing_df = pd.DataFrame()\n","    combined_df = pd.concat([existing_df, df], ignore_index=True).drop_duplicates(subset=['id', 'name', 'type', 'agentId', 'damages'])\n","    try:\n","        combined_df.to_csv(csv_file, index=False)\n","        print(f\"Abilities data saved to {csv_file}\")\n","    except Exception as e:\n","        print(f\"Failed to save abilities data: {e}\")"]},{"cell_type":"markdown","metadata":{},"source":["Function to process bracketJson for different types"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[],"source":["def process_bracket_json(bracketJson, bracket_title):\n","    series_ids = []\n","    bracket_type = bracketJson['type']\n","    if bracket_type == 'weekly':\n","        for week in bracketJson['weekly']['weeks']:\n","            for series in week['series']:\n","                series_ids.append(series['id'])\n","    elif bracket_type == 'double':\n","        for section in ['winners', 'losers']:\n","            for round_data in bracketJson[section]:\n","                for seed in round_data['seeds']:\n","                    series_ids.append(seed['seriesId'])\n","    elif bracket_type == 'single':\n","        for section in ['winners']:\n","            for round_data in bracketJson[section]:\n","                for seed in round_data['seeds']:\n","                    series_ids.append(seed['seriesId'])\n","    elif bracket_type == 'group':\n","        for group in bracketJson.get('groups', []):\n","            for seed in group.get('seeds', []):\n","                series_ids.append(seed.get('id'))\n","    else:\n","        print(f\"Unhandled bracket type: {bracket_type}\")\n","    return bracket_type, series_ids"]},{"cell_type":"markdown","metadata":{},"source":["Function to scrape tournament data"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[],"source":["def scrapeTourney(tourney_url):\n","    print(f\"Scraping tournament data from URL: {tourney_url}\")\n","    try:\n","        response = requests.get(tourney_url)\n","        response.raise_for_status()\n","    except requests.RequestException as e:\n","        print(f'Failed trying to scrape tournament data: {e}')\n","        return\n","    stats_data_raw = extract_json_data(response.text)\n","    child_events = stats_data_raw['props']['pageProps']['event']['childEvents']\n","    for event in child_events:\n","        event_title = sanitize_filename(event.get('name', 'unknown_event'))\n","        bracketJson = event.get('bracketJson', {})\n","        \n","        if not bracketJson:\n","            print(f\"No bracketJson found for event: {event_title}\")\n","            continue\n","        bracket_type, series_ids = process_bracket_json(bracketJson, event_title)\n","        for series_id in series_ids:\n","            if series_id in scraped_series_ids:\n","                print(f\"Series ID {series_id} already scraped. Skipping...\")\n","                continue\n","            bracket_folder = os.path.abspath(f'./{event_title}/{bracket_type}/{series_id}')\n","            os.makedirs(bracket_folder, exist_ok=True)\n","            print(f\"Created directory for series ID {series_id} at {bracket_folder}\")\n","            header_for_extra_data, match_ids = SeriesHeader(series_id)\n","            if header_for_extra_data is not None:\n","                update_abilities(header_for_extra_data)\n","                update_ign_and_id(header_for_extra_data)\n","                update_team(header_for_extra_data)\n","                for match_id in match_ids:\n","                    print(f\"Fetching details for match ID: {match_id}\")\n","                    try:\n","                        response = requests.get(f'https://be-prod.rib.gg/v1/matches/{match_id}/details')\n","                        response.raise_for_status()\n","                        details = response.json()\n","                        with open(f'{bracket_folder}/{match_id}_details.json', 'w') as json_file:\n","                            json.dump(details, json_file)\n","                        print(f\"Details for match ID {match_id} saved to {bracket_folder}\")\n","                    except requests.RequestException as e:\n","                        print(f'Failed to fetch details for match ID {match_id}: {e}')\n","                    except json.JSONDecodeError:\n","                        print(f\"Failed to decode JSON for match ID {match_id}\")\n","                    time.sleep(3)\n","            scraped_series_ids.append(series_id)\n","            save_scraped_series_ids(scraped_series_ids)"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[],"source":["def scrapeSeries(series_url):\n","    series_id = series_url.rstrip('/').split('/')[-1]\n","    if series_id in scraped_series_ids:\n","        print(f\"Series ID {series_id} already scraped. Skipping...\")\n","        return\n","    print(f\"Scraping data for series ID: {series_id}\")\n","    header_for_extra_data, match_ids = SeriesHeader(series_id)\n","    if header_for_extra_data is not None:\n","        # event_title = sanitize_filename(header_for_extra_data['props']['pageProps']['series']['title'])\n","        bracket_folder = os.path.abspath(f'./{series_id}')\n","        os.makedirs(bracket_folder, exist_ok=True)\n","        print(f\"Created directory for series ID {series_id} at {bracket_folder}\")\n","        update_abilities(header_for_extra_data)\n","        update_ign_and_id(header_for_extra_data)\n","        update_team(header_for_extra_data)\n","        for match_id in match_ids:\n","            print(f\"Fetching details for match ID: {match_id}\")\n","            try:\n","                response = requests.get(f'https://be-prod.rib.gg/v1/matches/{match_id}/details')\n","                response.raise_for_status()\n","                details = response.json()\n","                with open(f'{bracket_folder}/{match_id}_details.json', 'w') as json_file:\n","                    json.dump(details, json_file)\n","                print(f\"Details for match ID {match_id} saved to {bracket_folder}\")\n","            except requests.RequestException as e:\n","                print(f'Failed to fetch details for match ID {match_id}: {e}')\n","            except json.JSONDecodeError:\n","                print(f\"Failed to decode JSON for match ID {match_id}\")\n","            time.sleep(3)\n","        scraped_series_ids.append(series_id)\n","        save_scraped_series_ids(scraped_series_ids)"]},{"cell_type":"markdown","metadata":{},"source":["Uncomment to run scraping"]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[],"source":["# scrapeTourney(tourney_url)\n","# scrapeSeries(series_url)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.7"}},"nbformat":4,"nbformat_minor":2}
